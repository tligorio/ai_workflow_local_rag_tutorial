{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple AI Workflow with Local RAG using Ollama and Postgres\n",
    "#### Authored by Dr. Tiziana Ligorio for *AI Agents - CSCI 395.32* taught at Hunter College of The City University of New York\n",
    "\n",
    "In this demo, we build a simple AI workflow that demonstrates the **Retrieval Augmented Generation (RAG)** pattern — using a **fully local stack** with no API calls required.\n",
    "\n",
    "We will create a **FastHTML tutor** that can answer questions about the FastHTML library by retrieving relevant information from its documentation.\n",
    "\n",
    "Our workflow consists of the following stages:\n",
    "\n",
    "1. **Document Loading** — Load the FastHTML documentation from a text file\n",
    "2. **Chunking** — Split the documentation into manageable pieces\n",
    "3. **Embedding** — Create vector embeddings for each chunk using Ollama\n",
    "4. **Storage** — Store the embeddings in a local Postgres database with pgvector\n",
    "5. **Retrieval** — Query the database for relevant chunks based on user questions\n",
    "6. **Generation** — Use the retrieved context to generate accurate answers\n",
    "\n",
    "This tutorial mirrors the [hosted RAG tutorial (using OpenRouter and Supabase)](https://github.com/tligorio/ai_workflow_rag_tutorial/tree/main) but runs entirely on your local machine. This approach offers:\n",
    "- **Full privacy** — Your data never leaves your machine\n",
    "- **No API costs** — After initial setup, everything runs locally\n",
    "- **Offline capability** — Works without an internet connection\n",
    "\n",
    "We use Ollama for local LLM and embeddings, and Postgres with pgvector for vector storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites Setup\n",
    "\n",
    "Before running this notebook, you need to install and set up two components:\n",
    "1. **Ollama** — Local LLM server\n",
    "2. **Docker** — To run Postgres with pgvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 — Install and Set Up Ollama\n",
    "\n",
    "1. Download and install Ollama from https://ollama.com/download\n",
    "\n",
    "2. Start Ollama:\n",
    "   - **macOS**: Open Ollama from Applications (or Spotlight: Cmd+Space, type \"Ollama\"). A llama icon will appear in your menu bar.\n",
    "   - **Windows**: Open Ollama from the Start menu. A llama icon will appear in your system tray.\n",
    "   - **Linux**: Run `ollama serve` in a terminal (keep it running, or run as a background service).\n",
    "\n",
    "3. Open a terminal and pull the models we'll use:\n",
    "\n",
    "```bash\n",
    "# Pull a chat model\n",
    "ollama pull llama3.2\n",
    "\n",
    "# Pull an embedding model\n",
    "ollama pull mxbai-embed-large\n",
    "```\n",
    "\n",
    "4. Verify the models are available:\n",
    "\n",
    "```bash\n",
    "ollama list\n",
    "```\n",
    "\n",
    "You should see both models listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 — Install Docker and Start Postgres with pgvector\n",
    "\n",
    "1. Download and install Docker from https://docs.docker.com/get-docker/\n",
    "\n",
    "2. Start Docker:\n",
    "   - **macOS**: Open Docker from Applications (or Spotlight: Cmd+Space, type \"Docker\"). Wait for the whale icon in the menu bar to stop animating.\n",
    "   - **Windows**: Open Docker Desktop from the Start menu. Wait for the whale icon in the system tray to show \"Docker Desktop is running\".\n",
    "   - **Linux**: Run `sudo systemctl start docker` (or `sudo service docker start` on older systems).\n",
    "\n",
    "3. Start a Postgres container with pgvector:\n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "  --name pgvector \\\n",
    "  -e POSTGRES_USER=postgres \\\n",
    "  -e POSTGRES_PASSWORD=postgres \\\n",
    "  -e POSTGRES_DB=vectors \\\n",
    "  -p 5432:5432 \\\n",
    "  pgvector/pgvector:pg16\n",
    "```\n",
    "\n",
    "4. Verify it's running:\n",
    "\n",
    "```bash\n",
    "docker ps\n",
    "```\n",
    "\n",
    "You should see the `pgvector` container running.\n",
    "\n",
    "**Note:** To stop the container later: `docker stop pgvector`  \n",
    "To restart it: `docker start pgvector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installs and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install ollama psycopg2-binary python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%%capture` hides the pip install output to keep the notebook clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama: Python client for running local LLMs and embeddings\n",
    "import ollama\n",
    "\n",
    "# psycopg2: PostgreSQL adapter for Python\n",
    "import psycopg2\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import hashlib  # For generating content hashes to prevent duplicate document insertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ollama** is a tool for running large language models locally. It provides a simple API for chat completions and embeddings, similar to OpenAI's API but running entirely on your machine.\n",
    "\n",
    "**psycopg2** is the most popular PostgreSQL adapter for Python. We use it to connect to our local Postgres database with pgvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Clients\n",
    "\n",
    "We need to set up:\n",
    "1. **Ollama** — Already running as a service, we just use the `ollama` module\n",
    "2. **Postgres connection** — Connect to our local database\n",
    "\n",
    "We'll also define the models we'll use:\n",
    "- **Chat model**: `llama3.2` for generating responses\n",
    "- **Embedding model**: `mxbai-embed-large` for creating vector embeddings (1024 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running. Available models: ['nomic-embed-text:latest', 'llama3.2:latest']\n",
      "Postgres connection successful to vectors\n"
     ]
    }
   ],
   "source": [
    "# Database connection parameters\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"vectors\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"postgres\"\n",
    "\n",
    "# Models to use\n",
    "CHAT_MODEL = \"llama3.2\"\n",
    "EMBEDDING_MODEL = \"mxbai-embed-large\"\n",
    "EMBEDDING_DIM = 1024  # mxbai-embed-large produces 1024-dimensional vectors\n",
    "\n",
    "# Test Ollama connection\n",
    "try:\n",
    "    response = ollama.list()\n",
    "    print(f\"Ollama is running. Available models: {[m.model for m in response.models]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Ollama: {e}\")\n",
    "    print(\"Make sure Ollama is running (try 'ollama serve' in terminal)\")\n",
    "\n",
    "# Test Postgres connection\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD\n",
    "    )\n",
    "    conn.close()\n",
    "    print(f\"Postgres connection successful to {DB_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Postgres: {e}\")\n",
    "    print(\"Make sure the Docker container is running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Database\n",
    "\n",
    "We need to enable the pgvector extension and create our documents table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database setup complete: pgvector enabled and documents table created\n"
     ]
    }
   ],
   "source": [
    "def get_db_connection():\n",
    "    \"\"\"Create and return a database connection.\"\"\"\n",
    "    return psycopg2.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASSWORD\n",
    "    )\n",
    "\n",
    "# Enable pgvector extension and create table\n",
    "conn = get_db_connection()\n",
    "cur = conn.cursor() # Create a cursor\n",
    "\n",
    "# Enable the vector extension\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "\n",
    "# Create the documents table\n",
    "cur.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS documents (\n",
    "        id BIGSERIAL PRIMARY KEY,\n",
    "        content TEXT NOT NULL,\n",
    "        embedding vector({EMBEDDING_DIM}),\n",
    "        content_hash TEXT UNIQUE\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"Database setup complete: pgvector enabled and documents table created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Document Loading\n",
    "\n",
    "The first step in building a RAG system is loading the source documents. In our case, we have a single text file containing the FastHTML documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded document with 403766 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'### Minimal FastHTML Application Example\\n\\nSource: https://www.fastht.ml/docs/ref/concise_guide.html\\n\\nDemonstrates a basic FastHTML application setup. It includes defining an app instance, a route with'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the FastHTML documentation\n",
    "with open(\"FastHTML.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    document = f.read()\n",
    "\n",
    "print(f\"Loaded document with {len(document)} characters\")\n",
    "document[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Chunking\n",
    "\n",
    "Large documents need to be split into smaller pieces (chunks) for two reasons:\n",
    "\n",
    "1. **Embedding models have token limits** — Most embedding models can only process a limited amount of text at once\n",
    "2. **Retrieval precision** — Smaller chunks allow us to retrieve more relevant, focused context\n",
    "\n",
    "## Chunking Strategy\n",
    "\n",
    "We'll implement a simple **recursive character text splitter** to demonstrate how chunking works:\n",
    "- Split the text into chunks of approximately 1000 characters\n",
    "- Include 200 characters of overlap between chunks to preserve context across boundaries\n",
    "\n",
    "The overlap ensures that if important information spans a chunk boundary, it will appear in at least one complete chunk.\n",
    "\n",
    "**Note:** Since our FastHTML documentation is in markdown format, a better choice might be to use a markdown-aware splitter such as `MarkdownTextSplitter` from the `langchain-text-splitters` package. These splitters respect markdown structure (headers, code blocks, etc.) and can preserve header hierarchy as metadata. Here, we implement a simple character-based splitter to illustrate the core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to split\n",
    "        chunk_size: Target size for each chunk (in characters)\n",
    "        overlap: Number of characters to overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Get the chunk\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        \n",
    "        # Try to break at a natural boundary (newline or period)\n",
    "        if end < len(text):\n",
    "            # Look for a good break point near the end\n",
    "            last_newline = chunk.rfind('\\n')\n",
    "            last_period = chunk.rfind('. ')\n",
    "            \n",
    "            # Use the later of the two break points, if found in the last 20% of the chunk\n",
    "            break_point = max(last_newline, last_period)\n",
    "            if break_point > chunk_size * 0.8:\n",
    "                chunk = text[start:start + break_point + 1]\n",
    "                end = start + break_point + 1\n",
    "        \n",
    "        chunks.append(chunk.strip())\n",
    "        \n",
    "        # Move start position, accounting for overlap\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 527 chunks\n",
      "\n",
      "Example chunk (chunk 0):\n",
      "### Minimal FastHTML Application Example\n",
      "\n",
      "Source: https://www.fastht.ml/docs/ref/concise_guide.html\n",
      "\n",
      "Demonstrates a basic FastHTML application setup. It includes defining an app instance, a route with type-constrained parameters, and serving the application. The example highlights FastHTML's approach to routing, HTML generation using FastTags, and automatic server startup.\n",
      "\n",
      "```python\n",
      "# Meta-package with all key symbols from FastHTML and Starlette. Import it like this at the start of every FastHT...\n",
      "\n",
      "Chunk sizes: min=619, max=1000, avg=964\n"
     ]
    }
   ],
   "source": [
    "# Split the document into chunks\n",
    "chunks = chunk_text(document, chunk_size=1000, overlap=200)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(f\"\\nExample chunk (chunk 0):\\n{chunks[0][:500]}...\")\n",
    "print(f\"\\nChunk sizes: min={min(len(c) for c in chunks)}, max={max(len(c) for c in chunks)}, avg={sum(len(c) for c in chunks)//len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create Embeddings\n",
    "\n",
    "**Embeddings** are numerical representations of text that capture semantic meaning. Similar texts will have similar embeddings (vectors that are close together in high-dimensional space).\n",
    "\n",
    "We use Ollama's `mxbai-embed-large` model, which produces 1024-dimensional vectors. This model runs entirely locally and provides strong retrieval performance for technical content.\n",
    "\n",
    "## How Embeddings Work\n",
    "\n",
    "1. Text goes into the embedding model\n",
    "2. The model outputs a vector of 1024 floating-point numbers\n",
    "3. These numbers encode the semantic meaning of the text\n",
    "4. Similar meanings → similar vectors → can be found via similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"\n",
    "    Get the embedding vector for a piece of text using Ollama.\n",
    "    \n",
    "    Args:\n",
    "        text: The text to embed\n",
    "        \n",
    "    Returns:\n",
    "        A list of floats representing the embedding vector\n",
    "    \"\"\"\n",
    "    response = ollama.embed(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=text\n",
    "    )\n",
    "    return response.embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pedagoggical note:** Always understand what you are working with! Explore `response` directly to understand its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response fields: dict_keys(['model', 'created_at', 'done', 'done_reason', 'total_duration', 'load_duration', 'prompt_eval_count', 'prompt_eval_duration', 'eval_count', 'eval_duration', 'embeddings'])\n",
      "Number of embeddings: 1\n",
      "Embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "response = ollama.embed(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    input=\"What is FastHTML?\"\n",
    ")\n",
    "\n",
    "print(f\"Response fields: {response.model_dump().keys()}\")\n",
    "print(f\"Number of embeddings: {len(response.embeddings)}\")\n",
    "print(f\"Embedding dimension: {len(response.embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1024\n",
      "First 10 values: [-0.055831574, -0.045276552, -0.03962449, 0.058835875, -0.017381666, -0.0074752322, -0.0030449165, -0.024085538, 0.060129706, 0.029415373]\n"
     ]
    }
   ],
   "source": [
    "# Test the embedding function with a sample text\n",
    "sample_embedding = get_embedding(\"What is FastHTML?\")\n",
    "print(f\"Embedding dimension: {len(sample_embedding)}\")\n",
    "print(f\"First 10 values: {sample_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 527 chunks...\n",
      "  Processed 50/527 chunks\n",
      "  Processed 100/527 chunks\n",
      "  Processed 150/527 chunks\n",
      "  Processed 200/527 chunks\n",
      "  Processed 250/527 chunks\n",
      "  Processed 300/527 chunks\n",
      "  Processed 350/527 chunks\n",
      "  Processed 400/527 chunks\n",
      "  Processed 450/527 chunks\n",
      "  Processed 500/527 chunks\n",
      "Done! Generated 527 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all chunks\n",
    "# This may take a few minutes depending on the number of chunks\n",
    "print(f\"Generating embeddings for {len(chunks)} chunks...\")\n",
    "\n",
    "embeddings = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    embedding = get_embedding(chunk)\n",
    "    embeddings.append(embedding)\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(chunks)} chunks\")\n",
    "\n",
    "print(f\"Done! Generated {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Store in Postgres\n",
    "\n",
    "Now we store our chunks and their embeddings in our local Postgres database.\n",
    "\n",
    "We use `ON CONFLICT` to implement upsert behavior — if a document with the same content hash already exists, we update it rather than creating a duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting 527 documents into Postgres...\n",
      "  Processed 50/527 documents\n",
      "  Processed 100/527 documents\n",
      "  Processed 150/527 documents\n",
      "  Processed 200/527 documents\n",
      "  Processed 250/527 documents\n",
      "  Processed 300/527 documents\n",
      "  Processed 350/527 documents\n",
      "  Processed 400/527 documents\n",
      "  Processed 450/527 documents\n",
      "  Processed 500/527 documents\n",
      "Done! Upserted 527 documents into Postgres\n"
     ]
    }
   ],
   "source": [
    "def get_content_hash(text: str) -> str:\n",
    "    \"\"\"Generate a hash of the content to use as a unique identifier.\"\"\"\n",
    "    return hashlib.md5(text.encode()).hexdigest() # creates a hash as a 32-character hex string\n",
    "\n",
    "# Insert all chunks and embeddings into Postgres using upsert\n",
    "# This prevents duplicates if you re-run the notebook\n",
    "print(f\"Upserting {len(chunks)} documents into Postgres...\")\n",
    "\n",
    "conn = get_db_connection()\n",
    "cur = conn.cursor()\n",
    "\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "    content_hash = get_content_hash(chunk)\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO documents (content, embedding, content_hash)\n",
    "        VALUES (%s, %s, %s)\n",
    "        ON CONFLICT (content_hash) \n",
    "        DO UPDATE SET content = EXCLUDED.content, embedding = EXCLUDED.embedding;\n",
    "    \"\"\", (chunk, embedding, content_hash))\n",
    "    \n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"  Processed {i + 1}/{len(chunks)} documents\")\n",
    "        conn.commit()\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Done! Upserted {len(chunks)} documents into Postgres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents table contains 527 entries\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: verify documents are in the database\n",
    "conn = get_db_connection()\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT COUNT(*) FROM documents;\")\n",
    "count = cur.fetchone()[0]\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Documents table contains {count} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Retrieval\n",
    "\n",
    "Now we can search our vector database to find chunks relevant to a user's question.\n",
    "\n",
    "We use cosine distance (`<=>`) to measure similarity between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query: str, match_count: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Search for documents similar to the query.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        match_count: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of matching documents with their similarity scores\n",
    "    \"\"\"\n",
    "    # Get embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Search for similar documents\n",
    "    conn = get_db_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    \"\"\"\n",
    "    <=> is cosine distance operator (0 = identical, 2 = opposite)\n",
    "    %s::vector casts the Python list parameter to a Postgres vector type\n",
    "    1 - (embedding <=> %s::vector) as similarity - Converts distance to similarity (1 = identical, -1 = opposite)\n",
    "    \"\"\"\n",
    "    \n",
    "    cur.execute(\"\"\"\n",
    "        SELECT id, content, 1 - (embedding <=> %s::vector) as similarity\n",
    "        FROM documents\n",
    "        ORDER BY embedding <=> %s::vector\n",
    "        LIMIT %s;\n",
    "    \"\"\", (query_embedding, query_embedding, match_count))\n",
    "    \n",
    "    results = []\n",
    "    for row in cur.fetchall():\n",
    "        results.append({\n",
    "            \"id\": row[0],\n",
    "            \"content\": row[1],\n",
    "            \"similarity\": row[2]\n",
    "        })\n",
    "    \n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 matching documents:\n",
      "\n",
      "--- Result 1 (similarity: 0.7735) ---\n",
      "### Minimal FastHTML Application Example\n",
      "\n",
      "Source: https://www.fastht.ml/docs/ref/concise_guide.html\n",
      "\n",
      "Demonstrates a basic FastHTML application setup. It includes defining an app instance, a route with type-constrained parameters, and serving the application. The example highlights FastHTML's approac...\n",
      "\n",
      "--- Result 2 (similarity: 0.7589) ---\n",
      "als/by_example.html\n",
      "\n",
      "Demonstrates how to capture path parameters in FastHTML routes. The captured parameter 'nm' is used in the response string. It also shows how to test this route using TestClient.\n",
      "\n",
      "```python\n",
      "@app.get('/user/{nm}')\n",
      "def _(nm:str): return f\"Good day to you, {nm}!\"\n",
      "\n",
      "cli.get('/user/jp...\n",
      "\n",
      "--- Result 3 (similarity: 0.7585) ---\n",
      "----------------\n",
      "\n",
      "### Initialize FastHTML App and Router\n",
      "\n",
      "Source: https://www.fastht.ml/docs/tutorials/jupyter_and_fasthtml.html\n",
      "\n",
      "Sets up a new FastHTML application instance and its associated router. The `pico=True` argument suggests enabling a lightweight mode for the application.\n",
      "\n",
      "```python\n",
      "app, ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the search function\n",
    "results = search_documents(\"How do I create a route in FastHTML?\", match_count=3)\n",
    "\n",
    "print(f\"Found {len(results)} matching documents:\\n\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"--- Result {i+1} (similarity: {doc['similarity']:.4f}) ---\")\n",
    "    print(doc['content'][:300] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Generation\n",
    "\n",
    "Now we combine everything into a complete RAG pipeline:\n",
    "1. Take the user's question\n",
    "2. Retrieve relevant documents from our vector database\n",
    "3. Include those documents as context in the prompt\n",
    "4. Generate an answer using the LLM\n",
    "\n",
    "This is where the \"Augmented\" in Retrieval-Augmented Generation comes in — we augment the LLM's knowledge with retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_fasthtml_tutor(question: str, num_docs: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Ask the FastHTML tutor a question.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question about FastHTML\n",
    "        num_docs: Number of documents to retrieve for context\n",
    "        \n",
    "    Returns:\n",
    "        The tutor's response\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    docs = search_documents(question, match_count=num_docs)\n",
    "    \n",
    "    # Step 2: Build context from retrieved documents\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"content\"] for doc in docs])\n",
    "    \n",
    "    # Step 3: Create the prompt with context\n",
    "    system_prompt = \"\"\"You are a helpful FastHTML tutor. Answer questions about FastHTML based on the provided documentation context. \n",
    "\n",
    "If the context doesn't contain enough information to answer the question, say so honestly. \n",
    "Always be accurate and cite specific examples from the documentation when possible.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Context from FastHTML documentation:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a clear, helpful answer based on the documentation above.\"\"\"\n",
    "\n",
    "    # Step 4: Generate response using Ollama\n",
    "    response = ollama.chat(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do I create a route in FastHTML?\n",
      "\n",
      "Answer:\n",
      "Based on the provided documentation from FastHTML, you can create a route by using the `@rt` decorator or the alternative way of defining routes using `app.route`. \n",
      "\n",
      "Here are examples of both methods:\n",
      "\n",
      "**Method 1: Using the `@rt` Decorator**\n",
      "\n",
      "```python\n",
      "from fasthtml.common import *\n",
      "\n",
      "@app.get('/user/{nm}')\n",
      "def _(nm:str): return f\"Good day to you, {nm}!\"\n",
      "```\n",
      "\n",
      "In this example, `/user/{nm}` is a route that accepts an HTTP GET request and expects a path parameter `nm`. The function `_` takes the `nm` parameter as a string.\n",
      "\n",
      "**Method 2: Using `app.route`**\n",
      "\n",
      "```python\n",
      "from fasthtml.common import *\n",
      "from fastht.ml/docs/ref/concise_guide.html\n",
      "\n",
      "@app.route('/foo', methods=['GET','POST'])\n",
      "def foo(nm=name):\n",
      "    # Function implementation here\n",
      "```\n",
      "\n",
      "In this example, `/foo` is a route that accepts both GET and POST HTTP requests. The `nm` parameter is type-annotated and passed as a query parameter.\n",
      "\n",
      "Both of these examples demonstrate how to create routes in FastHTML.\n"
     ]
    }
   ],
   "source": [
    "# Test the complete RAG pipeline\n",
    "question = \"How do I create a route in FastHTML?\"\n",
    "answer = ask_fasthtml_tutor(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is another question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are FastTags in FastHTML?\n",
      "\n",
      "Answer:\n",
      "FastTags are direct m-expression mappings of HTML tags to Python functions with positional and named parameters. They allow you to define custom HTML elements using a concise syntax.\n",
      "\n",
      "In the documentation, it's mentioned that when a tuple is returned, this returns concatenated HTML partials. Additionally, FastHTML will automatically return a complete HTML document with appropriate headers if a normal HTTP request is received.\n",
      "\n",
      "For example, as shown in `files = {\"file1\": f1, \"file2\": (\"filename\", f2, \"image/png\")}`, the tuple returned from the function can result in concatenated HTML partials. Similarly, when defining FastTags like `Title`, `H1`, and `P` using the m-expression syntax, you are essentially creating functions that map to specific HTML tags.\n",
      "\n",
      "The documentation also mentions that Python reserved words used as attribute names require aliases. This implies that FastTags allow for a flexible way of mapping Python variables to HTML attributes or tag parameters.\n",
      "\n",
      "Overall, FastTags provide a powerful and concise way to work with HTML elements in your FastHTML applications, making it easier to build complex web interfaces without having to worry about the underlying HTML structure.\n"
     ]
    }
   ],
   "source": [
    "# Try your own question!\n",
    "my_question = \"What are FastTags in FastHTML?\"\n",
    "answer = ask_fasthtml_tutor(my_question)\n",
    "\n",
    "print(f\"Question: {my_question}\\n\")\n",
    "print(f\"Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Database Connections\n",
    "\n",
    "When you're done working with the database, it's good practice to ensure all connections are properly closed. While our helper functions open and close connections within each call, you should be mindful of connection management in production applications.\n",
    "\n",
    "Run the cell below to verify there are no lingering connections from this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents in database: 527\n",
      "Connection closed successfully\n"
     ]
    }
   ],
   "source": [
    "# Verify connection status and demonstrate proper cleanup\n",
    "# In our implementation, connections are opened and closed within each function,\n",
    "# but here's how you would explicitly close a connection if needed:\n",
    "\n",
    "def close_connection_safely(conn):\n",
    "    \"\"\"Safely close a database connection.\"\"\"\n",
    "    if conn is not None and not conn.closed:\n",
    "        conn.close()\n",
    "        print(\"Connection closed successfully\")\n",
    "    else:\n",
    "        print(\"No open connection to close\")\n",
    "\n",
    "# Example: Create a connection, use it, and close it properly\n",
    "test_conn = get_db_connection()\n",
    "cur = test_conn.cursor()\n",
    "cur.execute(\"SELECT COUNT(*) FROM documents;\")\n",
    "count = cur.fetchone()[0]\n",
    "print(f\"Documents in database: {count}\")\n",
    "cur.close()\n",
    "close_connection_safely(test_conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Ollama Models\n",
    "Ollama provides a library of open-source models you can run locally. Browse available models at:\n",
    "\n",
    "  https://ollama.com/library\n",
    "\n",
    "  Types of Models\n",
    "\n",
    "  1. **Chat/LLM Models** — For text generation and conversation. \n",
    "  - Examples: llama3.2, mistral, gemma2, phi3, qwen2.5\n",
    "  2. **Embedding Models** — For converting text to vectors (used in RAG)\n",
    "  - Examples: mxbai-embed-large, nomic-embed-text, bge-large, all-minilm\n",
    "  - Look for models with \"embed\" in the name\n",
    "  - To discover the embedding dimension, run:\n",
    "      ```\n",
    "      response = ollama.embed(model=\"model-name-here\", input=\"test\")\n",
    "      len(response.embeddings[0])\n",
    "      ```\n",
    "\n",
    "###   Understanding Model Variants\n",
    "\n",
    "  Models often come in different sizes and quantizations:\n",
    "\n",
    "  - **llama3.2**     - Default variant (usually the smallest recommended)\n",
    "  - **llama3.2:1b**      - 1 billion parameters (smaller, faster)\n",
    "  - **llama3.2:3b**      - 3 billion parameters (larger, smarter)\n",
    "  - **ll ama3.2:70b-q4**  - 70B parameters, 4-bit quantization (reduced precision for lower memory)\n",
    "\n",
    "  Larger models = Better quality but slower and more RAM\n",
    "  Quantization (q4, q8) = Compressed models that use less memory with slight quality trade-off\n",
    "\n",
    "###  Useful Commands\n",
    "```\n",
    "  ollama list              # Show installed models\n",
    "  ollama pull <model>      # Download a model\n",
    "  ollama rm <model>        # Delete a model\n",
    "  ollama show <model>      # Show model details (parameters, size, etc.)\n",
    "  ```\n",
    "\n",
    "### Documentation\n",
    "\n",
    "  - Ollama Documentation: https://github.com/ollama/ollama/blob/main/README.md\n",
    "  - Model Library: https://ollama.com/library\n",
    "  - Python API Reference: https://github.com/ollama/ollama-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resetting the Database\n",
    "\n",
    "If you want to re-run this notebook with a different embedding model, you'll need to delete the existing table first. This is because:\n",
    "\n",
    "1. **Different embedding models produce different vector dimensions** — The table schema is tied to a specific dimension.\n",
    "\n",
    "2. **Embeddings from different models are not comparable** — Even if two models have the same dimension, their embeddings encode meaning differently. Mixing embeddings from different models would produce nonsensical similarity results.\n",
    "\n",
    "**To reset and start fresh:**\n",
    "\n",
    "1. Run the cell below to drop the documents table\n",
    "2. Update `EMBEDDING_MODEL` and `EMBEDDING_DIM` in the configuration cell to match your new model\n",
    "3. Re-run the notebook from \"Initialize the Clients\" onward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents table dropped. You can now re-run the notebook with a different embedding model.\n"
     ]
    }
   ],
   "source": [
    "# Drop the documents table to start fresh\n",
    "# Uncomment and run this cell only when you want to reset the database\n",
    "\n",
    "# conn = get_db_connection()\n",
    "# cur = conn.cursor()\n",
    "# cur.execute(\"DROP TABLE IF EXISTS documents;\")\n",
    "# conn.commit()\n",
    "# cur.close()\n",
    "# conn.close()\n",
    "# print(\"Documents table dropped. You can now re-run the notebook with a different embedding model.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_workflow_local_rag_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
